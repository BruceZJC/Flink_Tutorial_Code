# 独立笔记-DEMO-Flink实现 批/流 处理WordCount

创建一个简单的 Flink Task，一个 Task 可以被提交给 Jobmanager，运行在 Taskmanager 上

### pom.xml依赖

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!-- pom.xml是maven工程的核心配置文件，与构建过程相关的一切配置都在这个文件中进行配置 -->
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.example</groupId>
    <artifactId>Practice</artifactId>
    <version>1.0-SNAPSHOT</version>
    <dependencies>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-java</artifactId>
            <version>1.13.1</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java_2.12</artifactId>
            <version>1.13.1</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients_2.11</artifactId>
            <version>1.13.1</version>
            <!-- After Flink 1.11, 
            the Flink streaming Java dependency on Flink clients has been removed, 
            and the clients dependency needs to be added manually 
            -->
        </dependency>

    </dependencies>

    <properties>
        <maven.compiler.source>11</maven.compiler.source>
        <maven.compiler.target>11</maven.compiler.target>
    </properties>

</project>
```

### 批处理代码实现

```java
package com.practice.wc;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.DataSet;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.operators.DataSource;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.util.Collector;

// 批处理 word count
public class wordCount {

    public static void main(String[] args) throws Exception {
        // 创建执行环境
        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

        //从文件中读取数据
        String inputPath = "C:\\Users\\86173\\IdeaProjects\\practice\\src\\main\\resources\\hello.txt";
        DataSource<String> inputDataset = env.readTextFile(inputPath);

        //对数据集进行处理，按照空格分词展开，转换成（word，1）二元组进行统计
        DataSet<Tuple2<String, Integer>> resultSet = inputDataset.flatMap( new myFlatMapper() )
                .groupBy(0) //按照第一个位置的word分组
                .sum(1); //将第二个位置上的数据求和
        resultSet.print();
    }

    // 自定义类，实现的是 FlatMap Function 结果
    // 类似Hadoop的Mapper功能，生成 K，V 对儿
    public static class myFlatMapper implements FlatMapFunction<String, Tuple2<String, Integer>>{

        @Override
        public void flatMap(String s, Collector<Tuple2<String, Integer>> collector) throws Exception {
            // 按空格分词
            String[] words = s.split(" ");

            //遍历所有word，包成二元组输出
            for (String w : words){
                collector.collect(new Tuple2<>(w, 1));
            }
        }
    }
}
```

### 流处理实现代码

- 批处理=>几组或所有数据到达后才处理；流处理=>有数据来就直接处理，不等数据堆叠到一定数量级
- 这里不像批处理有groupBy => 所有数据统一处理，而是用流处理的keyBy => 每一个数据都对key进行hash计算，进行类似分区的操作，来一个数据就处理一次，所有中间过程都有输出！
- 并行度：开发环境的并行度默认就是计算机的CPU逻辑核数

```java
package com.practice.wc;

import org.apache.flink.api.java.functions.KeySelector;
import org.apache.flink.api.java.operators.DataSource;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

public class streamWordCount {
    public static void main(String[] args) throws Exception{
        //创建流处理执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        //从文件中读取数据
        String inputPath = "C:\\Users\\86173\\IdeaProjects\\practice\\src\\main\\resources\\hello.txt";
        DataStream<String> inputDataStream = env.readTextFile(inputPath);

        //基于数据流进行转换计算
        DataStream<Tuple2<String, Integer>> resultStream = inputDataStream.flatMap(new wordCount.myFlatMapper())
                .keyBy( new myKeySelector())
                .sum(1);

        resultStream.print();

        //执行任务
        //这里env.execute();之前的代码，可以理解为是在定义任务，
        // 只有执行env.execute()后，Flink才把前面的代码片段当作一个任务整体
        //（每个线程根据这个任务操作，并行处理流数据）。
        env.execute();
    }

    public static class myKeySelector implements KeySelector<Tuple2<String,Integer>, String>{

        @Override
        public String getKey(Tuple2<String, Integer> value) throws Exception {
            return value.f0.toString();
        }
    }
}
```

将上方代码从读取文件的输入方式更改成读取用户自定义输入从而监听端口输入

```java
//        //从文件中读取数据
//        String inputPath = "C:\\Users\\86173\\IdeaProjects\\practice\\src\\main\\resources\\hello.txt";
//        DataStream<String> inputDataStream = env.readTextFile(inputPath);

        //用 Parameter tool 工具从程序启动参数中提取配置项
        ParameterTool parameterTool = ParameterTool.fromArgs(args);
        String host = parameterTool.get("host");
        int port = parameterTool.getInt("port");

        //从 Socket 文本流读取数据
        DataStream<String> inputDataStream = env.socketTextStream(host, port);
```